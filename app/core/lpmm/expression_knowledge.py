# app/core/lpmm/expression_knowledge.py
import os
import json
import datetime
import re
import numpy as np
import requests
import hashlib
from typing import List, Dict, Any, Optional

# ==================== é…ç½®åŠ è½½å·¥å…· ====================
def get_llm_config(config_key: str, default_config: dict = None) -> dict:
    if default_config is None:
        default_config = {}
    try:
        import toml
        config = toml.load("config/llm.toml")
        if config_key in config:
            return config[config_key]
        elif "." in config_key:
            main_key, sub_key = config_key.split(".", 1)
            if main_key in config and isinstance(config[main_key], dict) and sub_key in config[main_key]:
                return config[main_key][sub_key]
    except Exception:
        pass
    return default_config

def get_llm_config_value(config_key: str, value_key: str, default_value=None):
    config = get_llm_config(config_key, {})
    return config.get(value_key, default_value)

# ==================== åµŒå…¥æ¨¡å‹å·¥å…· ====================
def get_embed_provider_config(provider_name: str) -> dict:
    from dotenv import load_dotenv
    load_dotenv(".env", override=True)
    
    embed_configs = {
        "openai": {
            "url": os.getenv('openai_embed_url', 'https://api.openai.com/v1/embeddings'),
            "key": os.getenv('openai_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "text-embedding-ada-002")
        },
        "siliconflow": {
            "url": os.getenv('siliconflow_embed_url', 'https://api.siliconflow.cn/v1/embeddings'),
            "key": os.getenv('siliconflow_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "BAAI/bge-m3")
        },
        "local": {
            "url": os.getenv('local_embed_url'),
            "key": os.getenv('local_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "text-embedding")
        }
    }
    return embed_configs.get(provider_name, embed_configs["siliconflow"])

def get_embedding(text: str) -> Optional[List[float]]:
    try:
        embed_config = get_llm_config("lpmm.embedmodel", {})
        provider = embed_config.get("provider", "siliconflow")
        config = get_embed_provider_config(provider)
        
        url = config["url"]
        api_key = config["key"]
        model = config["model"]
        headers = config["headers"](api_key)
        
        payload = {
            "model": model,
            "input": text
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if "data" in data and len(data["data"]) > 0:
            if "embedding" in data["data"][0]:
                return data["data"][0]["embedding"]
            elif "embeddings" in data["data"][0]:
                return data["data"][0]["embeddings"]
        return None
    except Exception:
        return None

def cosine_similarity(vec1, vec2) -> float:
    if vec1 is None or vec2 is None:
        return 0.0
    a = np.array(vec1)
    b = np.array(vec2)
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def generate_file_hash(filepath: str) -> str:
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

# ==================== è¡¨è¾¾æ–¹å¼é€‰æ‹©å™¨ ====================
def should_learn_expression(text: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥å­¦ä¹ è¿™ä¸ªè¡¨è¾¾æ–¹å¼
    è§„åˆ™ï¼š
    1. é•¿åº¦åœ¨ 5-100 å­—ç¬¦ä¹‹é—´
    2. åŒ…å«æƒ…æ„Ÿè¯æˆ–ç½‘ç»œæµè¡Œè¯­
    3. ä¸æ˜¯çº¯æ ‡ç‚¹æˆ–æ•°å­—
    4. ä¸åŒ…å«æ•æ„Ÿè¯
    """
    if len(text) < 5 or len(text) > 100:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æƒ…æ„Ÿè¯æˆ–æµè¡Œè¯­
    emotional_words = [
        "å“ˆå“ˆ", "ç¬‘æ­»", "ç»äº†", "å¤ªå¯äº†", "awsl", "èšŒåŸ ä½äº†",
        "ç ´é˜²", "æ³ªç›®", "æ„ŸåŠ¨", "æš–å¿ƒ", "æ‰å¿ƒ", "çœŸå®", "å…¸ä¸­å…¸"
    ]
    has_emotional = any(word in text for word in emotional_words)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘ç»œæµè¡Œè¯­æ¨¡å¼
    internet_patterns = [
        r'[a-zA-Z]{2,}äº†$',  # xxäº†
        r'^[a-zA-Z]+$',      # çº¯å­—æ¯ç¼©å†™
        r'[0-9]+[a-zA-Z]+',  # æ•°å­—+å­—æ¯
        r'=.*=',             # å®šä¹‰å¼
    ]
    has_pattern = any(re.search(pattern, text) for pattern in internet_patterns)
    
    # æ£€æŸ¥æ˜¯å¦çº¯æ ‡ç‚¹/æ•°å­—
    if re.match(r'^[0-9\s\W]+$', text):
        return False
    
    # æ£€æŸ¥æ•æ„Ÿè¯
    sensitive_words = ["æ­»", "æ€", "æ»š", "éª‚", "æ“", "fuck", "shit"]
    if any(word in text.lower() for word in sensitive_words):
        return False
    
    return has_emotional or has_pattern

# ==================== è¡¨è¾¾æ–¹å¼çŸ¥è¯†åº“æ ¸å¿ƒç±» ====================
class ExpressionKnowledgeBase:
    def __init__(self, expr_dir: str = "lpmm-ie/expressions", json_dir: str = None):
        self.expr_dir = expr_dir
        self.json_dir = json_dir or os.path.join("lpmm-ie", "json", "expressions")
        self.data: List[Dict[str, Any]] = []  # âœ… ä¿®å¤ï¼šæ­£ç¡®å®šä¹‰ data å±æ€§
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.expr_dir, exist_ok=True)
        os.makedirs(self.json_dir, exist_ok=True)
        
        # ä»é…ç½®è¯»å–æ˜¯å¦åŠ è½½
        self.load_on_start = get_llm_config_value("lpmm.settings", "load_expressions_on_start", True)
        self.show_progress = get_llm_config_value("lpmm.settings", "show_progress", True)
        
        # åŠ è½½è¡¨è¾¾æ–¹å¼
        if self.load_on_start:
            self.reload()
        else:
            print("[è¡¨è¾¾æ–¹å¼åº“] è·³è¿‡å¯åŠ¨åŠ è½½ï¼ˆload_expressions_on_start=falseï¼‰")

    def extract_expressions_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„è¡¨è¾¾æ–¹å¼"""
        expressions = []
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # å¦‚æœæ˜¯å®šä¹‰å¼ï¼Œæå–ç­‰å·åéƒ¨åˆ†
            if "=" in line:
                parts = line.split('=', 1)
                if len(parts) == 2:
                    expr = parts[1].strip()
                    if expr and should_learn_expression(expr):
                        expressions.append(expr)
                    if parts[0].strip() and should_learn_expression(parts[0].strip()):
                        expressions.append(parts[0].strip())
            else:
                # æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', line)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if sentence and should_learn_expression(sentence):
                        expressions.append(sentence)
        
        return expressions

    def load_from_files(self) -> List[Dict[str, Any]]:
        """ä»è¡¨è¾¾æ–¹å¼æ–‡ä»¶åŠ è½½"""
        knowledge = []
        expr_files = [f for f in os.listdir(self.expr_dir) if f.endswith(".txt")]
        total_files = len(expr_files)
        if total_files == 0:
            return knowledge

        use_tqdm = self.show_progress
        if use_tqdm:
            try:
                from tqdm import tqdm
            except ImportError:
                use_tqdm = False
                print("[è¡¨è¾¾æ–¹å¼åº“] æœªå®‰è£… tqdmï¼Œè¿›åº¦æ¡å·²ç¦ç”¨")

        file_iter = expr_files
        if use_tqdm:
            file_iter = tqdm(expr_files, desc="ğŸ’¬ åŠ è½½è¡¨è¾¾æ–¹å¼", unit="æ–‡ä»¶")

        for filename in file_iter:
            path = os.path.join(self.expr_dir, filename)
            try:
                # ç”Ÿæˆæ–‡ä»¶å“ˆå¸Œ
                file_hash = generate_file_hash(path)
                cache_found = False
                cache_file = None

                # æ£€æŸ¥ç¼“å­˜
                for cache_filename in os.listdir(self.json_dir):
                    if cache_filename.endswith(".json") and filename.replace(".txt", "") in cache_filename:
                        cache_path = os.path.join(self.json_dir, cache_filename)
                        try:
                            with open(cache_path, 'r', encoding='utf-8') as f:
                                cache_data = json.load(f)
                                if isinstance(cache_data, list) and len(cache_data) > 0:
                                    if cache_data[0].get("source_file_hash") == file_hash:
                                        knowledge.extend(cache_data)
                                        cache_found = True
                                        if use_tqdm:
                                            file_iter.set_postfix({"çŠ¶æ€": "ç¼“å­˜å‘½ä¸­"})
                                        break
                        except:
                            continue

                if cache_found:
                    continue

                # é‡æ–°ç”Ÿæˆ
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()

                expressions = self.extract_expressions_from_text(content)
                file_knowledge = []
                
                expr_iter = expressions
                if use_tqdm and len(expressions) > 20:
                    from tqdm import tqdm
                    expr_iter = tqdm(expressions, desc=f"   {filename}", unit="æ¡", leave=False)

                for i, expr in enumerate(expr_iter):
                    if not should_learn_expression(expr):  # åŒé‡æ£€æŸ¥
                        continue
                        
                    embedding = get_embedding(expr)
                    item = {
                        "key": f"expr_{filename}_{i}",
                        "value": expr,
                        "embedding": embedding,
                        "tags": ["expression", "imported"],
                        "source": filename,
                        "source_file_hash": file_hash,
                        "created_at": datetime.datetime.now().isoformat(),
                        "usage_count": 0
                    }
                    file_knowledge.append(item)
                    knowledge.append(item)

                if file_knowledge:
                    self.save_to_json(file_knowledge, "expression", filename)

            except Exception as e:
                print(f"[è¡¨è¾¾æ–¹å¼åº“] åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return knowledge

    def save_to_json(self, knowledge_list: List[Dict], source_type: str, source_name: str = "") -> str:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{source_type}_{source_name.replace('.txt', '').replace('/', '_')}.json"
        filepath = os.path.join(self.json_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(knowledge_list, f, ensure_ascii=False, indent=2)
            return filepath
        except Exception as e:
            print(f"ä¿å­˜è¡¨è¾¾æ–¹å¼æ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def load_all_json(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰è¡¨è¾¾æ–¹å¼ç¼“å­˜"""
        knowledge = []
        if not os.path.exists(self.json_dir):
            return knowledge
        json_files = [f for f in os.listdir(self.json_dir) if f.endswith(".json")]
        use_tqdm = self.show_progress
        if use_tqdm:
            try:
                from tqdm import tqdm
            except ImportError:
                use_tqdm = False
        file_iter = json_files
        if use_tqdm:
            file_iter = tqdm(json_files, desc="ğŸ§  åŠ è½½è¡¨è¾¾ç¼“å­˜", unit="æ–‡ä»¶")
        for filename in file_iter:
            path = os.path.join(self.json_dir, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    items = json.load(f)
                    if isinstance(items, list):
                        knowledge.extend(items)
                    else:
                        knowledge.append(items)
            except Exception as e:
                print(f"åŠ è½½è¡¨è¾¾æ–¹å¼æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return knowledge

    def add_expression(self, expression: str, source: str = "auto_learn") -> Optional[Dict[str, Any]]:
        """æ·»åŠ æ–°è¡¨è¾¾æ–¹å¼"""
        if not should_learn_expression(expression):
            return None
            
        embedding = get_embedding(expression)
        new_item = {
            "key": f"expr_learned_{hash(expression) % 1000000}",
            "value": expression,
            "embedding": embedding,
            "tags": ["expression", "learned"],
            "source": source,
            "created_at": datetime.datetime.now().isoformat(),
            "usage_count": 1
        }
        self.save_to_json([new_item], "learned_expression", source)
        self.data.append(new_item)
        return new_item

    def reload(self):
        print("[è¡¨è¾¾æ–¹å¼åº“] å¼€å§‹åŠ è½½è¡¨è¾¾æ–¹å¼...")
        start_time = datetime.datetime.now()
        self.data = []
        json_knowledge = self.load_all_json()
        self.data.extend(json_knowledge)
        expr_knowledge = self.load_from_files()
        self.data.extend(expr_knowledge)
        end_time = datetime.datetime.now()
        duration = (end_time - start_time).total_seconds()
        print(f"[è¡¨è¾¾æ–¹å¼åº“] âœ… åŠ è½½å®Œæˆï¼å…± {len(self.data)} æ¡è¡¨è¾¾æ–¹å¼ï¼Œè€—æ—¶ {duration:.2f} ç§’")

    def search_similar(self, text: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼è¡¨è¾¾æ–¹å¼"""
        if not self.data or not text.strip():
            return []
            
        query_embedding = get_embedding(text)
        if query_embedding is None:
            return []
            
        matches = []
        for item in self.data:  # âœ… ä¿®å¤ï¼šself.data
            if item.get("embedding") is not None:
                score = cosine_similarity(query_embedding, item["embedding"])
                if score > 0.4:  # è¾ƒé«˜é˜ˆå€¼ï¼Œç¡®ä¿ç›¸ä¼¼åº¦
                    item_copy = item.copy()
                    item_copy["similarity"] = score
                    matches.append(item_copy)
        
        matches.sort(key=lambda x: x["similarity"], reverse=True)
        return matches[:top_k]

    def get_random_expressions(self, count: int = 3) -> List[str]:
        """éšæœºè·å–è¡¨è¾¾æ–¹å¼"""
        if not self.data:  # âœ… ä¿®å¤ï¼šself.data
            return []
        import random
        selected = random.sample(self.data, min(count, len(self.data)))
        return [item["value"] for item in selected]

    def manual_load(self):
        self.reload()

# ==================== å…¨å±€å•ä¾‹ ====================
_expression_kb_instance = None

def get_expression_knowledge_base() -> ExpressionKnowledgeBase:
    global _expression_kb_instance
    if _expression_kb_instance is None:
        _expression_kb_instance = ExpressionKnowledgeBase()
    return _expression_kb_instance

# ==================== ä¾¿æ·å‡½æ•° ====================
def search_similar_expressions(text: str, top_k: int = 3) -> List[Dict[str, Any]]:
    kb = get_expression_knowledge_base()
    return kb.search_similar(text, top_k)

def add_expression(expression: str, source: str = "auto_learn") -> Optional[Dict[str, Any]]:
    kb = get_expression_knowledge_base()
    return kb.add_expression(expression, source)

def get_random_expressions(count: int = 3) -> List[str]:
    kb = get_expression_knowledge_base()
    return kb.get_random_expressions(count)

def reload_expressions():
    kb = get_expression_knowledge_base()
    kb.reload()

def manual_load_expressions():
    kb = get_expression_knowledge_base()
    kb.manual_load()

# åˆå§‹åŒ–
if get_llm_config_value("lpmm.settings", "load_expressions_on_start", True):
    reload_expressions()
else:
    print("[è¡¨è¾¾æ–¹å¼åº“] å¯åŠ¨æ—¶æœªåŠ è½½è¡¨è¾¾æ–¹å¼ï¼ˆload_expressions_on_start=falseï¼‰")