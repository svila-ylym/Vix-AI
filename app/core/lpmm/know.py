# app/core/lpmm/know.py
import os
import json
import datetime
import re
import numpy as np
import requests
import hashlib
from typing import List, Dict, Any, Optional

# ==================== é…ç½®åŠ è½½å·¥å…· ====================
def get_llm_config(config_key: str, default_config: dict = None) -> dict:
    """ç®€åŒ–ç‰ˆé…ç½®åŠ è½½ï¼ˆä¾›çŸ¥è¯†åº“ç‹¬ç«‹ä½¿ç”¨ï¼‰"""
    if default_config is None:
        default_config = {}
    try:
        import toml
        config = toml.load("config/llm.toml")
        if config_key in config:
            return config[config_key]
        elif "." in config_key:
            main_key, sub_key = config_key.split(".", 1)
            if main_key in config and isinstance(config[main_key], dict) and sub_key in config[main_key]:
                return config[main_key][sub_key]
    except Exception:
        pass
    return default_config

def get_llm_config_value(config_key: str, value_key: str, default_value=None):
    config = get_llm_config(config_key, {})
    return config.get(value_key, default_value)

# ==================== åµŒå…¥æ¨¡å‹å·¥å…· ====================
def get_embed_provider_config(provider_name: str) -> dict:
    """è·å–åµŒå…¥æ¨¡å‹æä¾›å•†é…ç½®"""
    from dotenv import load_dotenv
    load_dotenv(".env", override=True)
    
    embed_configs = {
        "openai": {
            "url": os.getenv('openai_embed_url', 'https://api.openai.com/v1/embeddings'),
            "key": os.getenv('openai_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "text-embedding-ada-002")
        },
        "siliconflow": {
            "url": os.getenv('siliconflow_embed_url', 'https://api.siliconflow.cn/v1/embeddings'),
            "key": os.getenv('siliconflow_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "BAAI/bge-m3")
        },
        "local": {
            "url": os.getenv('local_embed_url'),
            "key": os.getenv('local_api_key'),
            "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
            "model": get_llm_config_value("lpmm.embedmodel", "model", "text-embedding")
        }
    }
    return embed_configs.get(provider_name, embed_configs["siliconflow"])

def get_embedding(text: str) -> Optional[List[float]]:
    """è°ƒç”¨åµŒå…¥æ¨¡å‹è·å–å‘é‡"""
    try:
        embed_config = get_llm_config("lpmm.embedmodel", {})
        provider = embed_config.get("provider", "siliconflow")
        config = get_embed_provider_config(provider)
        
        url = config["url"]
        api_key = config["key"]
        model = config["model"]
        headers = config["headers"](api_key)
        
        payload = {
            "model": model,
            "input": text
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if "data" in data and len(data["data"]) > 0:
            if "embedding" in data["data"][0]:
                return data["data"][0]["embedding"]
            elif "embeddings" in data["data"][0]:
                return data["data"][0]["embeddings"]
        return None
    except Exception:
        return None

def cosine_similarity(vec1, vec2) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    if vec1 is None or vec2 is None:
        return 0.0
    a = np.array(vec1)
    b = np.array(vec2)
    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def generate_file_hash(filepath: str) -> str:
    """ç”Ÿæˆæ–‡ä»¶å†…å®¹çš„ MD5 å“ˆå¸Œï¼Œç”¨äºç‰ˆæœ¬æ£€æµ‹"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

# ==================== çŸ¥è¯†åº“æ ¸å¿ƒç±» ====================
class KnowledgeBase:
    def __init__(self, text_dir: str = "lpmm-ie", json_dir: str = None):
        self.text_dir = text_dir
        self.json_dir = json_dir or os.path.join(text_dir, "json")
        self.data : List[Dict[str, Any]] = []  # âœ… æ­£ç¡®å®šä¹‰ data å±æ€§
        os.makedirs(self.text_dir, exist_ok=True)
        os.makedirs(self.json_dir, exist_ok=True)
        
        self.load_on_start = get_llm_config_value("lpmm.settings", "load_on_start", True)
        self.show_progress = get_llm_config_value("lpmm.settings", "show_progress", True)
        
        if self.load_on_start:
            self.reload()
        else:
            print("[çŸ¥è¯†åº“] è·³è¿‡å¯åŠ¨åŠ è½½ï¼ˆload_on_start=falseï¼‰")

    def split_sentences(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å¥ï¼šä¼˜å…ˆæŒ‰æ¢è¡Œï¼Œå…¶æ¬¡æŒ‰æ ‡ç‚¹ï¼Œä¿ç•™ç­‰å·ç»“æ„"""
        sentences = []
        
        # å¦‚æœåŒ…å«ç­‰å·ä¸”æ²¡æœ‰æ ‡ç‚¹ï¼Œæ•´å¥ä¿ç•™ï¼ˆé€‚é…å®šä¹‰å¼æ–‡æœ¬ï¼‰
        if "=" in text and not any(c in text for c in "ã€‚ï¼ï¼Ÿï¼›"):
            sentences.append(text.strip())
            return sentences
        
        # æŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # å¦‚æœæ˜¯å®šä¹‰å¼ï¼ˆå«ç­‰å·ï¼‰ï¼Œç›´æ¥ä¿ç•™
            if "=" in line and len(line.split('=', 1)) == 2:
                sentences.append(line)
            else:
                # æŒ‰æ ‡ç‚¹åˆ‡åˆ†
                parts = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', line)
                for part in parts:
                    part = part.strip()
                    if len(part) >= 3:  # æ”¾å®½åˆ°3ä¸ªå­—ç¬¦
                        sentences.append(part)
        
        return sentences

    def load_from_files(self) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½çŸ¥è¯† - ä¼˜åŒ–ç‰ˆ"""
        knowledge = []
        txt_files = [f for f in os.listdir(self.text_dir) if f.endswith(".txt")]
        total_files = len(txt_files)
        if total_files == 0:
            return knowledge

        use_tqdm = self.show_progress
        if use_tqdm:
            try:
                from tqdm import tqdm
            except ImportError:
                use_tqdm = False
                print("[çŸ¥è¯†åº“] æœªå®‰è£… tqdmï¼Œè¿›åº¦æ¡å·²ç¦ç”¨ã€‚è¿è¡Œ: pip install tqdm")

        file_iter = txt_files
        if use_tqdm:
            file_iter = tqdm(txt_files, desc="ğŸ“„ å¤„ç†çŸ¥è¯†æ–‡ä»¶", unit="æ–‡ä»¶")

        for filename in file_iter:
            path = os.path.join(self.text_dir, filename)
            try:
                # ç”Ÿæˆæ–‡ä»¶å“ˆå¸Œ
                file_hash = generate_file_hash(path)
                cache_found = False
                cache_file = None

                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”ç¼“å­˜æ–‡ä»¶
                for cache_filename in os.listdir(self.json_dir):
                    if cache_filename.endswith(".json") and filename.replace(".txt", "") in cache_filename:
                        cache_path = os.path.join(self.json_dir, cache_filename)
                        try:
                            with open(cache_path, 'r', encoding='utf-8') as f:
                                cache_data = json.load(f)
                                if isinstance(cache_data, list) and len(cache_data) > 0:
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶å“ˆå¸Œï¼ˆç‰ˆæœ¬æ ¡éªŒï¼‰
                                    if cache_data[0].get("source_file_hash") == file_hash:
                                        # âœ… ç¼“å­˜å‘½ä¸­ï¼ç›´æ¥åŠ è½½
                                        knowledge.extend(cache_data)
                                        cache_found = True
                                        if use_tqdm:
                                            file_iter.set_postfix({"çŠ¶æ€": "ç¼“å­˜å‘½ä¸­"})
                                        break
                        except:
                            continue

                if cache_found:
                    continue

                # âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œé‡æ–°ç”Ÿæˆ
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()

                lines = content.split('\n')
                file_knowledge = []
                line_iter = lines
                if use_tqdm and len(lines) > 50:
                    from tqdm import tqdm
                    line_iter = tqdm(lines, desc=f"   {filename}", unit="è¡Œ", leave=False)

                for i, line in enumerate(line_iter):
                    line = line.strip()
                    if not line:
                        continue
                    if "=" in line:
                        clean_text = line
                    else:
                        sentences = self.split_sentences(line)
                        if not sentences:
                            sentences = [line]
                        clean_text = sentences[0] if sentences else line
                    if len(clean_text.strip()) < 3:
                        continue
                    
                    # è·å–åµŒå…¥
                    embedding = get_embedding(clean_text)
                    item = {
                        "key": f"{filename}_{i}",
                        "value": clean_text.strip(),
                        "embedding": embedding,
                        "tags": ["imported", "txt"],
                        "source": filename,
                        "source_file_hash": file_hash,  # âœ… è®°å½•æ–‡ä»¶å“ˆå¸Œç”¨äºç¼“å­˜æ ¡éªŒ
                        "created_at": datetime.datetime.now().isoformat(),
                        "usage_count": 0
                    }
                    file_knowledge.append(item)
                    knowledge.append(item)

                # ä¿å­˜æ–°ç¼“å­˜
                if file_knowledge:
                    self.save_to_json(file_knowledge, "imported", filename)

            except Exception as e:
                print(f"[çŸ¥è¯†åº“] åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return knowledge

    def save_to_json(self, knowledge_list: List[Dict], source_type: str, source_name: str = "") -> str:
        """ä¿å­˜çŸ¥è¯†åˆ°JSONæ–‡ä»¶"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{source_type}_{source_name.replace('.txt', '').replace('/', '_')}.json"
        filepath = os.path.join(self.json_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(knowledge_list, f, ensure_ascii=False, indent=2)
            return filepath
        except Exception as e:
            print(f"ä¿å­˜çŸ¥è¯†æ–‡ä»¶å¤±è´¥: {e}")
            return ""

    def load_all_json(self) -> List[Dict[str, Any]]:
        """ä»JSONç›®å½•åŠ è½½æ‰€æœ‰çŸ¥è¯†"""
        knowledge = []
        if not os.path.exists(self.json_dir):
            return knowledge
        json_files = [f for f in os.listdir(self.json_dir) if f.endswith(".json")]
        use_tqdm = self.show_progress
        if use_tqdm:
            try:
                from tqdm import tqdm
            except ImportError:
                use_tqdm = False
        file_iter = json_files
        if use_tqdm:
            file_iter = tqdm(json_files, desc="ğŸ§  åŠ è½½çŸ¥è¯†ç¼“å­˜", unit="æ–‡ä»¶")
        for filename in file_iter:
            path = os.path.join(self.json_dir, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    items = json.load(f)
                    if isinstance(items, list):
                        knowledge.extend(items)
                    else:
                        knowledge.append(items)
            except Exception as e:
                print(f"åŠ è½½çŸ¥è¯†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
        return knowledge

    def add_knowledge(self, key: str, value: str, tags: List[str] = None) -> Dict[str, Any]:
        """æ·»åŠ æ–°çŸ¥è¯†"""
        if tags is None:
            tags = []
        embedding = get_embedding(value)
        new_item = {
            "key": key.strip(),
            "value": value.strip(),
            "embedding": embedding,
            "tags": [t.strip() for t in tags] + ["learned"],
            "source": "auto_learn",
            "created_at": datetime.datetime.now().isoformat(),
            "usage_count": 1
        }
        self.save_to_json([new_item], "learned", key.replace(" ", "_"))
        self.data.append(new_item)
        return new_item

    def reload(self):
        """é‡æ–°åŠ è½½æ‰€æœ‰çŸ¥è¯†"""
        print("[çŸ¥è¯†åº“] å¼€å§‹åŠ è½½çŸ¥è¯†...")
        start_time = datetime.datetime.now()
        self.data = []
        
        # å…ˆåŠ è½½ç‹¬ç«‹å­¦ä¹ çš„çŸ¥è¯†ï¼ˆJSONï¼‰
        json_knowledge = self.load_all_json()
        self.data.extend(json_knowledge)
        
        # å†åŠ è½½TXTæ–‡ä»¶ï¼ˆå¸¦ç¼“å­˜æ£€æµ‹ï¼‰
        txt_knowledge = self.load_from_files()
        self.data.extend(txt_knowledge)
        
        end_time = datetime.datetime.now()
        duration = (end_time - start_time).total_seconds()
        print(f"[çŸ¥è¯†åº“] âœ… åŠ è½½å®Œæˆï¼å…± {len(self.data)} æ¡çŸ¥è¯†ï¼Œè€—æ—¶ {duration:.2f} ç§’")

    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢ + å…³é”®è¯å›é€€"""
        if not self.data or not query.strip():
            return []
        query_embedding = get_embedding(query)
        semantic_matches = []
        if query_embedding is not None:
            for item in self.data:  # âœ… ä¿®å¤ï¼šself.data
                if item.get("embedding") is not None:
                    score = cosine_similarity(query_embedding, item["embedding"])
                    if score > 0.3:
                        item_copy = item.copy()
                        item_copy["score"] = score
                        item_copy["match_type"] = "semantic"
                        semantic_matches.append(item_copy)
            semantic_matches.sort(key=lambda x: x["score"], reverse=True)
            if len(semantic_matches) >= top_k:
                return semantic_matches[:top_k]
        keyword_matches = []
        query_lower = query.lower().strip()
        for item in self.data:  # âœ… ä¿®å¤ï¼šself.data
            score = 0
            item_value_lower = item["value"].lower()
            item_key_lower = item["key"].lower()
            if query_lower in item_key_lower:
                score += 2
            if query_lower in item_value_lower:
                score += 1
            if query_lower == item_value_lower:
                score += 3
            if score > 0:
                item_copy = item.copy()
                item_copy["score"] = score
                item_copy["match_type"] = "keyword"
                keyword_matches.append(item_copy)
        keyword_matches.sort(key=lambda x: x["score"], reverse=True)
        all_matches = semantic_matches + [m for m in keyword_matches if m not in semantic_matches]
        
        # å°è¯•æ·»åŠ è¡¨è¾¾æ–¹å¼æ¨è
        try:
            from .expression_knowledge import search_similar_expressions
            expr_matches = search_similar_expressions(query, top_k=2)
            for expr in expr_matches:
                expr_item = {
                    "key": "expression_recommendation",
                    "value": expr["value"],
                    "score": expr["similarity"] * 0.8,  # é™ä½æƒé‡
                    "match_type": "expression",
                    "source": "expression_knowledge"
                }
                all_matches.append(expr_item)
        except Exception:
            pass
        
        all_matches.sort(key=lambda x: x["score"], reverse=True)
        return all_matches[:top_k]

    def get_all(self) -> List[Dict[str, Any]]:
        return self.data.copy()

    def manual_load(self):
        self.reload()

# ==================== å…¨å±€å•ä¾‹ ====================
_knowledge_base_instance = None

def get_knowledge_base() -> KnowledgeBase:
    global _knowledge_base_instance
    if _knowledge_base_instance is None:
        _knowledge_base_instance = KnowledgeBase()
    return _knowledge_base_instance

# ==================== ä¾¿æ·å‡½æ•° ====================
def search_knowledge(query: str, top_k: int = 3) -> List[Dict[str, Any]]:
    kb = get_knowledge_base()
    return kb.search(query, top_k)

def add_knowledge(key: str, value: str, tags: List[str] = None) -> Dict[str, Any]:
    kb = get_knowledge_base()
    return kb.add_knowledge(key, value, tags)

def reload_knowledge():
    kb = get_knowledge_base()
    kb.reload()

def manual_load_knowledge():
    kb = get_knowledge_base()
    kb.manual_load()

# åˆå§‹åŒ–
if get_llm_config_value("lpmm.settings", "load_on_start", True):
    reload_knowledge()
else:
    print("[çŸ¥è¯†åº“] å¯åŠ¨æ—¶æœªåŠ è½½çŸ¥è¯†åº“ï¼ˆload_on_start=falseï¼‰")
    print("[æç¤º] å¦‚éœ€æ‰‹åŠ¨åŠ è½½ï¼Œè¯·è°ƒç”¨ manual_load_knowledge()")